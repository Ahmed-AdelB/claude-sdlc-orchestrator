#!/bin/bash
#===============================================================================
# tri-24-monitor - Active Monitor Daemon with Heartbeat (INC-ARCH-001)
#===============================================================================
# Features:
# - INC-ARCH-001.1: Send heartbeat every 30 seconds to state/monitor-heartbeat
# - INC-ARCH-001.2: Actively poll worker status instead of passive logging
# - INC-ARCH-001.3: Detect and alert on worker failures within 60 seconds
# - INC-ARCH-001.4: Auto-restart crashed workers
# - INC-ARCH-001.5: Integration with SQLite for persistent monitoring
#
# Architecture:
# - Main loop runs every 10 seconds for responsive failure detection
# - Heartbeat written every 30 seconds with comprehensive system state
# - Worker health checks via SQLite + tmux + process verification
# - Automatic recovery actions with cooldown to prevent thrashing
# - Event logging to SQLite for audit trail
#===============================================================================
set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
ROOT_DIR="${SCRIPT_DIR}/.."
LOG_DIR="${ROOT_DIR}/logs"
STATE_DIR="${ROOT_DIR}/state"

# Source common utilities if available
if [[ -f "${ROOT_DIR}/lib/common.sh" ]]; then
    # shellcheck source=/dev/null
    source "${ROOT_DIR}/lib/common.sh"
fi

# Source SQLite state if available
if [[ -f "${ROOT_DIR}/lib/sqlite-state.sh" ]]; then
    # shellcheck source=/dev/null
    source "${ROOT_DIR}/lib/sqlite-state.sh"
fi

# Source heartbeat utilities if available
if [[ -f "${ROOT_DIR}/lib/heartbeat.sh" ]]; then
    # shellcheck source=/dev/null
    source "${ROOT_DIR}/lib/heartbeat.sh"
fi

#===============================================================================
# CONFIGURATION (INC-ARCH-001)
#===============================================================================
MONITOR_LOG="${LOG_DIR}/tri-24-monitor.log"
ACTIVITY_LOG="${LOG_DIR}/tri-24-activity.jsonl"
METRICS_LOG="${LOG_DIR}/tri-24-metrics.jsonl"
PID_FILE="${STATE_DIR}/monitor.pid"
HEARTBEAT_FILE="${STATE_DIR}/monitor-heartbeat"
ALERTS_LOG="${LOG_DIR}/tri-24-alerts.jsonl"

# Timing configuration (INC-ARCH-001)
POLL_INTERVAL="${POLL_INTERVAL:-10}"               # Main loop interval (seconds)
HEARTBEAT_INTERVAL="${HEARTBEAT_INTERVAL:-30}"      # Heartbeat write interval (seconds)
FAILURE_DETECTION_WINDOW="${FAILURE_DETECTION_WINDOW:-60}"  # Worker failure detection (seconds)
RECOVERY_COOLDOWN="${RECOVERY_COOLDOWN:-120}"       # Cooldown between recovery attempts (seconds)
WORKER_HEARTBEAT_TIMEOUT="${WORKER_HEARTBEAT_TIMEOUT:-90}"  # Worker heartbeat timeout (seconds)

# SQLite configuration
STATE_DB="${STATE_DB:-${STATE_DIR}/tri-agent.db}"

# Recovery state
declare -A LAST_RECOVERY_TIME
declare -i TOTAL_RECOVERIES=0
declare -i ALERTS_SENT=0
declare -i MONITOR_START_TIME=0
declare -i LAST_HEARTBEAT_TIME=0
declare -i CONSECUTIVE_FAILURES=0
MONITOR_STATE="starting"

mkdir -p "$LOG_DIR" "$STATE_DIR"

#===============================================================================
# Error handling and crash recovery
#===============================================================================
_monitor_error_handler() {
    local exit_code="$1"
    local line_no="$2"
    local command="${3:-unknown}"
    local ts
    ts=$(date -Iseconds)

    # Log error before exit
    echo "[$ts][ERROR] Monitor daemon crashed at line $line_no: '$command' (exit code: $exit_code)" >> "$MONITOR_LOG"

    # Write crash event to activity log
    echo "{\"timestamp\":\"$ts\",\"type\":\"daemon_crash\",\"details\":{\"exit_code\":$exit_code,\"line\":$line_no,\"command\":\"$command\"}}" >> "$ACTIVITY_LOG"

    # Write to stderr for debugging
    >&2 echo "[CRASH] tri-24-monitor crashed at line $line_no: $command (exit $exit_code)"

    # Record to SQLite if available
    if command -v sqlite3 &>/dev/null && [[ -f "$STATE_DB" ]]; then
        sqlite3 "$STATE_DB" "INSERT INTO events (event_type, actor, payload, trace_id) VALUES ('MONITOR_CRASH', 'tri-24-monitor', '{\"exit_code\":$exit_code,\"line\":$line_no}', '${TRACE_ID:-}');" 2>/dev/null || true
    fi
}

_monitor_cleanup_handler() {
    local ts
    ts=$(date -Iseconds)
    echo "[$ts][INFO] Monitor daemon shutting down (PID $$)" >> "$MONITOR_LOG"

    # Update state
    MONITOR_STATE="stopped"
    _update_heartbeat

    # Update SQLite state
    if command -v sqlite3 &>/dev/null && [[ -f "$STATE_DB" ]]; then
        sqlite3 "$STATE_DB" "INSERT OR REPLACE INTO health_status (component, status, details, updated_at) VALUES ('monitor', 'stopped', '{\"pid\":$$,\"shutdown\":\"graceful\"}', datetime('now'));" 2>/dev/null || true
    fi

    # Remove PID file on clean exit
    rm -f "$PID_FILE" 2>/dev/null || true
}

# Set traps for error handling
trap '_monitor_error_handler $? $LINENO "$BASH_COMMAND"' ERR
trap '_monitor_cleanup_handler' EXIT SIGTERM SIGINT

# Write PID file for crash detection by guardian
echo $$ > "$PID_FILE"
MONITOR_START_TIME=$(date +%s)

#===============================================================================
# Colors
#===============================================================================
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
NC='\033[0m'

#===============================================================================
# LOGGING (INC-ARCH-001)
#===============================================================================
log() {
    local level="$1"
    shift
    local msg="$*"
    local ts
    ts=$(date -Iseconds)
    echo "[$ts][$level] $msg" | tee -a "$MONITOR_LOG"
}

record_activity() {
    local activity_type="$1"
    local details="$2"
    local ts
    ts=$(date -Iseconds)
    echo "{\"timestamp\":\"$ts\",\"type\":\"$activity_type\",\"details\":$details}" >> "$ACTIVITY_LOG"
}

record_alert() {
    local alert_type="$1"
    local severity="$2"
    local details="$3"
    local ts
    ts=$(date -Iseconds)

    echo "{\"timestamp\":\"$ts\",\"alert_type\":\"$alert_type\",\"severity\":\"$severity\",\"details\":$details}" >> "$ALERTS_LOG"
    ((ALERTS_SENT++))

    # Record to SQLite
    if command -v sqlite3 &>/dev/null && [[ -f "$STATE_DB" ]]; then
        local esc_type="${alert_type//\'/\'\'}"
        local esc_details="${details//\'/\'\'}"
        sqlite3 "$STATE_DB" "INSERT INTO events (event_type, actor, payload, trace_id) VALUES ('ALERT_${esc_type}', 'tri-24-monitor', '${esc_details}', '${TRACE_ID:-}');" 2>/dev/null || true
    fi

    log "ALERT" "[$severity] $alert_type: $details"
}

record_metrics() {
    local ts
    ts=$(date -Iseconds)
    local cpu mem queue running completed failed
    cpu=$(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | cut -d'%' -f1 2>/dev/null || echo "0")
    mem=$(free | grep Mem | awk '{printf("%.1f", $3/$2 * 100.0)}' 2>/dev/null || echo "0")
    queue=$(find "$ROOT_DIR/tasks/queue" -name "*.md" 2>/dev/null | wc -l || echo "0")
    running=$(find "$ROOT_DIR/tasks/running" -name "*.md" 2>/dev/null | wc -l || echo "0")
    completed=$(find "$ROOT_DIR/tasks/completed" -name "*.md" 2>/dev/null | wc -l || echo "0")
    failed=$(find "$ROOT_DIR/tasks/failed" -name "*.md" 2>/dev/null | wc -l || echo "0")

    echo "{\"timestamp\":\"$ts\",\"cpu\":\"$cpu\",\"mem\":\"$mem\",\"tasks\":{\"queue\":$queue,\"running\":$running,\"completed\":$completed,\"failed\":$failed}}" >> "$METRICS_LOG"
}

#===============================================================================
# INC-ARCH-001.1: HEARTBEAT SYSTEM
#===============================================================================
# Sends comprehensive heartbeat every 30 seconds to state/monitor-heartbeat
# Contains full system state for external monitoring tools
#===============================================================================
_update_heartbeat() {
    local now ts uptime
    now=$(date +%s)
    ts=$(date -Iseconds)
    uptime=$((now - MONITOR_START_TIME))

    # Gather worker status
    local workers_json
    workers_json=$(_get_workers_status_json)

    # Gather task counts
    local queue_count running_count completed_count failed_count
    queue_count=$(find "$ROOT_DIR/tasks/queue" -name "*.md" 2>/dev/null | wc -l || echo "0")
    running_count=$(find "$ROOT_DIR/tasks/running" -name "*.md" 2>/dev/null | wc -l || echo "0")
    completed_count=$(find "$ROOT_DIR/tasks/completed" -name "*.md" 2>/dev/null | wc -l || echo "0")
    failed_count=$(find "$ROOT_DIR/tasks/failed" -name "*.md" 2>/dev/null | wc -l || echo "0")

    # Write comprehensive heartbeat
    cat > "$HEARTBEAT_FILE" <<EOF
{
  "timestamp": "$ts",
  "epoch": $now,
  "pid": $$,
  "state": "$MONITOR_STATE",
  "uptime_seconds": $uptime,
  "config": {
    "poll_interval": $POLL_INTERVAL,
    "heartbeat_interval": $HEARTBEAT_INTERVAL,
    "failure_detection_window": $FAILURE_DETECTION_WINDOW,
    "recovery_cooldown": $RECOVERY_COOLDOWN,
    "worker_heartbeat_timeout": $WORKER_HEARTBEAT_TIMEOUT
  },
  "stats": {
    "total_recoveries": $TOTAL_RECOVERIES,
    "alerts_sent": $ALERTS_SENT,
    "consecutive_failures": $CONSECUTIVE_FAILURES
  },
  "tasks": {
    "queue": $queue_count,
    "running": $running_count,
    "completed": $completed_count,
    "failed": $failed_count
  },
  "workers": $workers_json,
  "tmux_session": "$(tmux has-session -t tri-24 2>/dev/null && echo "active" || echo "inactive")"
}
EOF

    LAST_HEARTBEAT_TIME=$now
}

#===============================================================================
# INC-ARCH-001.2: ACTIVE WORKER STATUS POLLING
#===============================================================================
# Actively polls worker status from multiple sources:
# 1. SQLite workers table (primary)
# 2. tmux window content (secondary)
# 3. Process table (verification)
#===============================================================================

_get_workers_status_json() {
    local workers_json="[]"

    if command -v sqlite3 &>/dev/null && [[ -f "$STATE_DB" ]]; then
        # Query workers from SQLite
        workers_json=$(sqlite3 -json "$STATE_DB" "
            SELECT
                worker_id,
                pid,
                status,
                started_at,
                last_heartbeat,
                tasks_completed,
                tasks_failed,
                shard,
                model
            FROM workers
            WHERE status NOT IN ('dead', 'offline')
            ORDER BY worker_id;
        " 2>/dev/null || echo "[]")

        # Fallback if -json not supported
        if [[ "$workers_json" == "[]" || -z "$workers_json" ]]; then
            workers_json="["
            local first=1
            while IFS='|' read -r worker_id pid status started_at last_heartbeat tasks_completed tasks_failed shard model; do
                [[ -z "$worker_id" ]] && continue
                [[ $first -eq 0 ]] && workers_json+=","
                first=0
                workers_json+="{\"worker_id\":\"$worker_id\",\"pid\":$pid,\"status\":\"$status\",\"last_heartbeat\":\"$last_heartbeat\"}"
            done < <(sqlite3 "$STATE_DB" "SELECT worker_id, COALESCE(pid,0), status, started_at, last_heartbeat, tasks_completed, tasks_failed, shard, model FROM workers WHERE status NOT IN ('dead', 'offline');" 2>/dev/null)
            workers_json+="]"
        fi
    fi

    echo "$workers_json"
}

check_tmux_session() {
    if tmux has-session -t tri-24 2>/dev/null; then
        local windows
        windows=$(tmux list-windows -t tri-24 2>/dev/null | wc -l)
        echo "$windows"
    else
        echo "0"
    fi
}

# INC-ARCH-001.2: Active worker polling from tmux
poll_worker_from_tmux() {
    local worker_num="$1"
    local status="unknown"
    local activity=""

    if tmux has-session -t tri-24 2>/dev/null; then
        local content
        content=$(tmux capture-pane -t "tri-24:$worker_num" -p 2>/dev/null | tail -15 || echo "")

        if echo "$content" | grep -qE "Processing|Claiming|Executing"; then
            status="busy"
            activity=$(echo "$content" | grep -oE "(Processing|Claiming|Executing)[^$]*" | tail -1 || echo "")
        elif echo "$content" | grep -qE "Idle|Waiting|Ready"; then
            status="idle"
        elif echo "$content" | grep -qiE "Error|FATAL|CRASH|Traceback|Exception"; then
            status="error"
            activity=$(echo "$content" | grep -iE "Error|FATAL|CRASH|Traceback|Exception" | tail -1 || echo "")
        elif echo "$content" | grep -qE "Worker|Started"; then
            status="running"
        else
            status="unknown"
        fi
    fi

    echo "$status"
}

# INC-ARCH-001.2: Active polling of all workers
poll_all_workers() {
    local healthy_workers=0
    local unhealthy_workers=0
    local worker_statuses=""

    # Check workers from SQLite
    if command -v sqlite3 &>/dev/null && [[ -f "$STATE_DB" ]]; then
        local now
        now=$(date +%s)

        while IFS='|' read -r worker_id pid status last_heartbeat; do
            [[ -z "$worker_id" ]] && continue

            local worker_healthy=1
            local reason=""

            # Check heartbeat age
            if [[ -n "$last_heartbeat" && "$last_heartbeat" != "NULL" ]]; then
                local heartbeat_epoch
                heartbeat_epoch=$(date -d "$last_heartbeat" +%s 2>/dev/null || echo "0")
                local heartbeat_age=$((now - heartbeat_epoch))

                if (( heartbeat_age > WORKER_HEARTBEAT_TIMEOUT )); then
                    worker_healthy=0
                    reason="heartbeat_stale_${heartbeat_age}s"
                fi
            else
                worker_healthy=0
                reason="no_heartbeat"
            fi

            # Verify process is alive
            if [[ -n "$pid" && "$pid" != "NULL" && "$pid" =~ ^[0-9]+$ ]]; then
                if ! kill -0 "$pid" 2>/dev/null; then
                    worker_healthy=0
                    reason="process_dead"
                fi
            fi

            # Check status
            if [[ "$status" == "dead" || "$status" == "stopping" || "$status" == "stale" ]]; then
                worker_healthy=0
                reason="status_$status"
            fi

            if [[ $worker_healthy -eq 1 ]]; then
                ((healthy_workers++))
            else
                ((unhealthy_workers++))
                worker_statuses+="$worker_id:$reason,"
            fi

        done < <(sqlite3 "$STATE_DB" "SELECT worker_id, pid, status, last_heartbeat FROM workers WHERE status NOT IN ('offline');" 2>/dev/null)
    fi

    # Also check tmux windows as fallback
    for win in 1 2 3; do
        local tmux_status
        tmux_status=$(poll_worker_from_tmux "$win")
        if [[ "$tmux_status" == "error" ]]; then
            worker_statuses+="tmux-worker-$win:error,"
        fi
    done

    echo "${healthy_workers}:${unhealthy_workers}:${worker_statuses}"
}

check_supervisor_health() {
    if ! tmux has-session -t tri-24 2>/dev/null; then
        return 1
    fi

    # Check supervisor window (0) for activity
    local output
    output=$(tmux capture-pane -t "tri-24:0" -p 2>/dev/null | tail -20 || echo "")

    # Look for error patterns
    if echo "$output" | grep -qiE "FATAL|CRASH|PANIC|Traceback"; then
        return 1
    fi

    return 0
}

#===============================================================================
# INC-ARCH-001.3: FAILURE DETECTION (within 60 seconds)
#===============================================================================
# Detects worker failures by checking:
# 1. SQLite heartbeat timestamps
# 2. Process liveness (kill -0)
# 3. tmux window content for error patterns
# 4. Task state (stuck in RUNNING too long)
#===============================================================================

detect_worker_failures() {
    local now failures_detected
    now=$(date +%s)
    failures_detected=0

    if ! command -v sqlite3 &>/dev/null || [[ ! -f "$STATE_DB" ]]; then
        # Fallback to tmux-only detection
        for win in 1 2 3; do
            local tmux_status
            tmux_status=$(poll_worker_from_tmux "$win")
            if [[ "$tmux_status" == "error" || "$tmux_status" == "unknown" ]]; then
                ((failures_detected++))
                record_alert "WORKER_FAILURE" "WARNING" "{\"worker\":\"tmux-window-$win\",\"status\":\"$tmux_status\",\"source\":\"tmux\"}"
            fi
        done
        echo "$failures_detected"
        return
    fi

    # Check workers from SQLite
    while IFS='|' read -r worker_id pid status last_heartbeat current_task; do
        [[ -z "$worker_id" ]] && continue

        local failure_reason=""

        # Check 1: Heartbeat timeout
        if [[ -n "$last_heartbeat" && "$last_heartbeat" != "NULL" ]]; then
            local heartbeat_epoch
            heartbeat_epoch=$(date -d "$last_heartbeat" +%s 2>/dev/null || echo "0")
            local heartbeat_age=$((now - heartbeat_epoch))

            if (( heartbeat_age > FAILURE_DETECTION_WINDOW )); then
                failure_reason="heartbeat_timeout(${heartbeat_age}s)"
            fi
        else
            failure_reason="no_heartbeat"
        fi

        # Check 2: Process liveness (only if we have a PID and no failure yet)
        if [[ -z "$failure_reason" && -n "$pid" && "$pid" != "NULL" && "$pid" =~ ^[0-9]+$ ]]; then
            if ! kill -0 "$pid" 2>/dev/null; then
                failure_reason="process_dead(pid=$pid)"
            fi
        fi

        # Check 3: Status indicates failure
        if [[ -z "$failure_reason" ]]; then
            if [[ "$status" == "dead" || "$status" == "stale" ]]; then
                failure_reason="status_$status"
            fi
        fi

        # Record failure if detected
        if [[ -n "$failure_reason" ]]; then
            ((failures_detected++))

            # Only alert if not already dead (avoid alert spam)
            if [[ "$status" != "dead" ]]; then
                record_alert "WORKER_FAILURE" "CRITICAL" "{\"worker_id\":\"$worker_id\",\"pid\":$pid,\"reason\":\"$failure_reason\",\"current_task\":\"${current_task:-null}\"}"

                # Mark worker as dead in SQLite
                local esc_worker="${worker_id//\'/\'\'}"
                sqlite3 "$STATE_DB" "UPDATE workers SET status='dead', last_heartbeat=datetime('now') WHERE worker_id='${esc_worker}';" 2>/dev/null || true
            fi
        fi

    done < <(sqlite3 "$STATE_DB" "
        SELECT
            w.worker_id,
            w.pid,
            w.status,
            w.last_heartbeat,
            (SELECT id FROM tasks t WHERE t.worker_id = w.worker_id AND t.state = 'RUNNING' LIMIT 1) as current_task
        FROM workers w
        WHERE w.status NOT IN ('offline', 'stopping');
    " 2>/dev/null)

    echo "$failures_detected"
}

# Check for stuck tasks (running too long)
detect_stuck_tasks() {
    local stuck_count=0
    local max_task_age="${MAX_TASK_AGE:-3600}"  # 1 hour default

    if command -v sqlite3 &>/dev/null && [[ -f "$STATE_DB" ]]; then
        while IFS='|' read -r task_id worker_id started_at age_seconds; do
            [[ -z "$task_id" ]] && continue

            if (( age_seconds > max_task_age )); then
                ((stuck_count++))
                record_alert "STUCK_TASK" "WARNING" "{\"task_id\":\"$task_id\",\"worker_id\":\"${worker_id:-unknown}\",\"age_seconds\":$age_seconds}"
            fi
        done < <(sqlite3 "$STATE_DB" "
            SELECT
                id,
                worker_id,
                started_at,
                CAST((julianday('now') - julianday(started_at)) * 86400 AS INTEGER) as age_seconds
            FROM tasks
            WHERE state = 'RUNNING'
              AND started_at IS NOT NULL;
        " 2>/dev/null)
    fi

    echo "$stuck_count"
}

#===============================================================================
# INC-ARCH-001.4: AUTO-RESTART CRASHED WORKERS
#===============================================================================
# Automatically restarts workers that have crashed with:
# - Cooldown period to prevent thrashing
# - Recovery via tmux send-keys
# - SQLite state cleanup
# - Event logging for audit trail
#===============================================================================

can_recover_worker() {
    local worker_id="$1"
    local now
    now=$(date +%s)

    local last_recovery="${LAST_RECOVERY_TIME[$worker_id]:-0}"
    local time_since_recovery=$((now - last_recovery))

    if (( time_since_recovery < RECOVERY_COOLDOWN )); then
        log "DEBUG" "Worker $worker_id in recovery cooldown (${time_since_recovery}s < ${RECOVERY_COOLDOWN}s)"
        return 1
    fi

    return 0
}

recover_worker() {
    local worker_num="$1"
    local worker_id="${2:-worker-$worker_num}"
    local reason="${3:-unknown}"
    local now
    now=$(date +%s)

    # Check cooldown
    if ! can_recover_worker "$worker_id"; then
        log "WARN" "Skipping recovery for $worker_id - in cooldown period"
        return 1
    fi

    log "INFO" "Attempting to recover worker $worker_id (window: $worker_num, reason: $reason)"

    if ! tmux has-session -t tri-24 2>/dev/null; then
        log "ERROR" "Cannot recover worker - tri-24 session not found"
        return 1
    fi

    # Determine model based on worker number
    local model
    case "$worker_num" in
        1) model="claude" ;;
        2) model="codex" ;;
        3) model="gemini" ;;
        *) model="claude" ;;
    esac

    # Clean up SQLite state for this worker
    if command -v sqlite3 &>/dev/null && [[ -f "$STATE_DB" ]]; then
        local esc_worker="${worker_id//\'/\'\'}"

        # Release any tasks claimed by this worker back to queue
        sqlite3 "$STATE_DB" "
            BEGIN IMMEDIATE;

            -- Requeue tasks from dead worker
            UPDATE tasks
            SET state='QUEUED',
                worker_id=NULL,
                updated_at=datetime('now'),
                retry_count = COALESCE(retry_count, 0) + 1
            WHERE worker_id='${esc_worker}' AND state='RUNNING';

            -- Log recovery event
            INSERT INTO events (event_type, actor, payload, trace_id)
            VALUES ('WORKER_RECOVERY', 'tri-24-monitor',
                    '{\"worker_id\":\"${esc_worker}\",\"reason\":\"$reason\",\"model\":\"$model\"}',
                    '${TRACE_ID:-}');

            -- Remove old worker entry
            DELETE FROM workers WHERE worker_id='${esc_worker}';

            COMMIT;
        " 2>/dev/null || true
    fi

    # Send restart command to tmux window
    tmux send-keys -t "tri-24:$worker_num" C-c 2>/dev/null || true
    sleep 1

    local new_worker_id="worker-${model}-$((worker_num-1))"
    tmux send-keys -t "tri-24:$worker_num" "export WORKER_ID='$new_worker_id' WORKER_SHARD='$((worker_num-1))' WORKER_MODEL='$model' && cd '$ROOT_DIR' && ./bin/tri-agent-worker" Enter 2>/dev/null || {
        log "ERROR" "Failed to send restart command to tmux window $worker_num"
        return 1
    }

    # Update recovery tracking
    LAST_RECOVERY_TIME[$worker_id]=$now
    ((TOTAL_RECOVERIES++))

    log "INFO" "Sent restart command to worker window $worker_num (new ID: $new_worker_id)"
    record_activity "worker_recovery" "{\"worker_num\":$worker_num,\"old_worker_id\":\"$worker_id\",\"new_worker_id\":\"$new_worker_id\",\"model\":\"$model\",\"reason\":\"$reason\"}"

    # Also move any files from running back to queue
    local running_dir="${ROOT_DIR}/tasks/running"
    local queue_dir="${ROOT_DIR}/tasks/queue"

    if [[ -d "$running_dir" ]]; then
        for task_file in "$running_dir"/*.md; do
            [[ -f "$task_file" ]] || continue
            local task_name
            task_name=$(basename "$task_file")
            local lock_dir="${running_dir}/${task_name}.lock.d"

            # Check if lock belongs to the recovered worker
            if [[ -d "$lock_dir" ]]; then
                local lock_worker_id=""
                [[ -f "$lock_dir/worker_id" ]] && lock_worker_id=$(cat "$lock_dir/worker_id" 2>/dev/null)

                if [[ "$lock_worker_id" == "$worker_id" ]]; then
                    rm -rf "$lock_dir" 2>/dev/null || true
                    mv "$task_file" "$queue_dir/" 2>/dev/null || true
                    log "INFO" "Requeued task: $task_name (from recovered worker $worker_id)"
                fi
            fi
        done
    fi

    return 0
}

# Auto-recover all failed workers
auto_recover_failed_workers() {
    local recovered=0

    # Get list of failed workers from SQLite
    if command -v sqlite3 &>/dev/null && [[ -f "$STATE_DB" ]]; then
        while IFS='|' read -r worker_id pid status shard; do
            [[ -z "$worker_id" ]] && continue

            # Determine window number from shard
            local worker_num
            if [[ -n "$shard" && "$shard" =~ ^[0-9]+$ ]]; then
                worker_num=$((shard + 1))
            else
                # Try to infer from worker_id
                case "$worker_id" in
                    *claude*|*-0) worker_num=1 ;;
                    *codex*|*-1) worker_num=2 ;;
                    *gemini*|*-2) worker_num=3 ;;
                    *) worker_num=1 ;;
                esac
            fi

            if recover_worker "$worker_num" "$worker_id" "auto_recovery_$status"; then
                ((recovered++))
            fi

        done < <(sqlite3 "$STATE_DB" "SELECT worker_id, pid, status, shard FROM workers WHERE status IN ('dead', 'stale');" 2>/dev/null)
    fi

    # Also check tmux windows for crashed workers
    for win in 1 2 3; do
        local tmux_status
        tmux_status=$(poll_worker_from_tmux "$win")

        if [[ "$tmux_status" == "error" || "$tmux_status" == "unknown" ]]; then
            local worker_id="tmux-worker-$win"
            if can_recover_worker "$worker_id"; then
                if recover_worker "$win" "$worker_id" "tmux_${tmux_status}"; then
                    ((recovered++))
                fi
            fi
        fi
    done

    echo "$recovered"
}

#===============================================================================
# INC-ARCH-001.5: SQLITE INTEGRATION
#===============================================================================
# Persistent monitoring data stored in SQLite:
# - health_status table for monitor state
# - events table for audit trail
# - workers table for worker state
#===============================================================================

update_monitor_status_sqlite() {
    if ! command -v sqlite3 &>/dev/null || [[ ! -f "$STATE_DB" ]]; then
        return 0
    fi

    local now
    now=$(date +%s)
    local uptime=$((now - MONITOR_START_TIME))

    local status_json
    status_json=$(cat <<EOF
{
  "pid": $$,
  "state": "$MONITOR_STATE",
  "uptime_seconds": $uptime,
  "total_recoveries": $TOTAL_RECOVERIES,
  "alerts_sent": $ALERTS_SENT,
  "consecutive_failures": $CONSECUTIVE_FAILURES
}
EOF
)

    sqlite3 "$STATE_DB" "
        INSERT OR REPLACE INTO health_status (component, status, details, updated_at)
        VALUES ('monitor', '$MONITOR_STATE', '${status_json//\'/\'\'}', datetime('now'));
    " 2>/dev/null || true
}

# Initialize SQLite tables if needed
init_sqlite_monitoring() {
    if ! command -v sqlite3 &>/dev/null; then
        log "WARN" "sqlite3 not available - SQLite monitoring disabled"
        return 1
    fi

    # Initialize database if needed
    if declare -F sqlite_state_init &>/dev/null; then
        sqlite_state_init "$STATE_DB" 2>/dev/null || true
    fi

    # Ensure health_status table exists
    sqlite3 "$STATE_DB" "
        CREATE TABLE IF NOT EXISTS health_status (
            component TEXT PRIMARY KEY,
            status TEXT,
            details TEXT,
            updated_at TEXT DEFAULT (datetime('now'))
        );
    " 2>/dev/null || true

    log "INFO" "SQLite monitoring initialized: $STATE_DB"
    return 0
}

#===============================================================================
# MAIN MONITORING LOOP
#===============================================================================
monitor_loop() {
    local interval="${1:-$POLL_INTERVAL}"
    local iteration=0

    log "INFO" "Starting TRI-24 Active Monitor Daemon (INC-ARCH-001)"
    log "INFO" "  Poll interval: ${interval}s"
    log "INFO" "  Heartbeat interval: ${HEARTBEAT_INTERVAL}s"
    log "INFO" "  Failure detection window: ${FAILURE_DETECTION_WINDOW}s"
    log "INFO" "  Recovery cooldown: ${RECOVERY_COOLDOWN}s"
    log "INFO" "  Worker heartbeat timeout: ${WORKER_HEARTBEAT_TIMEOUT}s"
    log "INFO" "  Activity log: $ACTIVITY_LOG"
    log "INFO" "  Alerts log: $ALERTS_LOG"
    log "INFO" "  Metrics log: $METRICS_LOG"

    # Initialize SQLite monitoring
    init_sqlite_monitoring || true

    MONITOR_STATE="running"
    _update_heartbeat

    while true; do
        ((iteration++))
        local now
        now=$(date +%s)

        # =====================================================
        # INC-ARCH-001.1: Update heartbeat every 30 seconds
        # =====================================================
        if (( now - LAST_HEARTBEAT_TIME >= HEARTBEAT_INTERVAL )); then
            _update_heartbeat
            log "DEBUG" "Heartbeat updated (iteration: $iteration)"
        fi

        # =====================================================
        # Check tmux session exists
        # =====================================================
        local windows
        windows=$(check_tmux_session)

        if [[ "$windows" == "0" ]]; then
            log "ERROR" "TRI-24 tmux session NOT FOUND!"
            record_alert "SESSION_MISSING" "CRITICAL" "{\"windows\":0,\"action\":\"waiting_for_guardian\"}"
            CONSECUTIVE_FAILURES=$((CONSECUTIVE_FAILURES + 1))
            MONITOR_STATE="degraded"
        else
            # =====================================================
            # INC-ARCH-001.2: Active worker polling
            # =====================================================
            local poll_result
            poll_result=$(poll_all_workers)
            IFS=':' read -r healthy_workers unhealthy_workers worker_issues <<< "$poll_result"

            # =====================================================
            # INC-ARCH-001.3: Failure detection
            # =====================================================
            local failures_detected stuck_tasks
            failures_detected=$(detect_worker_failures)
            stuck_tasks=$(detect_stuck_tasks)

            # Check supervisor health
            local supervisor_healthy=1
            if ! check_supervisor_health; then
                supervisor_healthy=0
                record_alert "SUPERVISOR_UNHEALTHY" "WARNING" "{\"source\":\"tmux_content_analysis\"}"
            fi

            # Log status
            if [[ "$unhealthy_workers" -gt 0 || "$failures_detected" -gt 0 ]]; then
                log "WARN" "Health check: windows=$windows, healthy=$healthy_workers, unhealthy=$unhealthy_workers, failures=$failures_detected, stuck=$stuck_tasks"
                CONSECUTIVE_FAILURES=$((CONSECUTIVE_FAILURES + 1))

                # =====================================================
                # INC-ARCH-001.4: Auto-restart crashed workers
                # =====================================================
                if [[ "$failures_detected" -gt 0 ]]; then
                    local recovered
                    recovered=$(auto_recover_failed_workers)
                    if [[ "$recovered" -gt 0 ]]; then
                        log "INFO" "Auto-recovered $recovered worker(s)"
                        CONSECUTIVE_FAILURES=0
                    fi
                fi
            else
                log "INFO" "TRI-24 healthy: windows=$windows, workers=$healthy_workers, supervisor=$supervisor_healthy"
                CONSECUTIVE_FAILURES=0
                MONITOR_STATE="running"
            fi

            record_activity "health_check" "{\"windows\":$windows,\"healthy_workers\":${healthy_workers:-0},\"unhealthy_workers\":${unhealthy_workers:-0},\"failures\":${failures_detected:-0},\"stuck_tasks\":${stuck_tasks:-0},\"supervisor_healthy\":$supervisor_healthy}"
        fi

        # Record metrics
        record_metrics

        # =====================================================
        # INC-ARCH-001.5: Update SQLite status
        # =====================================================
        update_monitor_status_sqlite

        # Detailed status report every 5 iterations
        if (( iteration % 5 == 0 )); then
            log "INFO" "=== Detailed Status Report (iteration $iteration) ==="

            # Task counts
            local queue running completed failed
            queue=$(find "$ROOT_DIR/tasks/queue" -name "*.md" 2>/dev/null | wc -l || echo "0")
            running=$(find "$ROOT_DIR/tasks/running" -name "*.md" 2>/dev/null | wc -l || echo "0")
            completed=$(find "$ROOT_DIR/tasks/completed" -name "*.md" 2>/dev/null | wc -l || echo "0")
            failed=$(find "$ROOT_DIR/tasks/failed" -name "*.md" 2>/dev/null | wc -l || echo "0")
            log "INFO" "Tasks: queue=$queue, running=$running, completed=$completed, failed=$failed"
            log "INFO" "Monitor stats: recoveries=$TOTAL_RECOVERIES, alerts=$ALERTS_SENT, consecutive_failures=$CONSECUTIVE_FAILURES"

            # Trigger stale task recovery if available
            if declare -F recover_stale_tasks &>/dev/null; then
                local recovered
                recovered=$(recover_stale_tasks 2>/dev/null || echo "0")
                [[ "$recovered" -gt 0 ]] && log "INFO" "Recovered $recovered stale tasks"
            fi
        fi

        sleep "$interval"
    done
}

#===============================================================================
# MAIN ENTRY POINT
#===============================================================================
echo -e "${BLUE}========================================${NC}"
echo -e "${BLUE}  TRI-24 Active Monitor Daemon${NC}"
echo -e "${BLUE}  INC-ARCH-001 Implementation${NC}"
echo -e "${BLUE}========================================${NC}"
echo ""
echo -e "Monitor Log:   ${GREEN}$MONITOR_LOG${NC}"
echo -e "Activity Log:  ${GREEN}$ACTIVITY_LOG${NC}"
echo -e "Alerts Log:    ${GREEN}$ALERTS_LOG${NC}"
echo -e "Metrics Log:   ${GREEN}$METRICS_LOG${NC}"
echo -e "Heartbeat:     ${GREEN}$HEARTBEAT_FILE${NC}"
echo -e "PID File:      ${GREEN}$PID_FILE${NC}"
echo ""
echo -e "Configuration:"
echo -e "  Poll Interval:            ${CYAN}${POLL_INTERVAL}s${NC}"
echo -e "  Heartbeat Interval:       ${CYAN}${HEARTBEAT_INTERVAL}s${NC}"
echo -e "  Failure Detection Window: ${CYAN}${FAILURE_DETECTION_WINDOW}s${NC}"
echo -e "  Recovery Cooldown:        ${CYAN}${RECOVERY_COOLDOWN}s${NC}"
echo -e "  Worker Heartbeat Timeout: ${CYAN}${WORKER_HEARTBEAT_TIMEOUT}s${NC}"
echo ""

monitor_loop "${1:-$POLL_INTERVAL}"
